{"Operations/Ansible/ansible-handler":{"title":"Ansible 中的 Handler","links":[],"tags":["ansible","learn_with_gpt"],"content":"Ansible 中的 handlers 是一種特殊類型的 task，只有在被通知（notify）時才會執行。通常用於需要在多個 task 之後執行的操作，比如重新啟動服務、重新載入設置文件等。這種設計可以避免不必要的重複操作，提高運行效率。\nHandlers 的工作方式\n\n定義 handlers：你需要在 playbook 或 role 中定義 handlers，這些 handlers 與普通的 tasks 一樣，只是它們會被特殊處理。\n通知 handlers：在需要觸發 handlers 的 task 中使用 notify 關鍵字來通知相應的 handler。\n執行 handlers：當所有 tasks 執行完畢後，Ansible 會執行被通知的 handlers。\n\n---\n- name: Install and configure web server\n  hosts: webservers\n  become: yes\n \n  tasks:\n    - name: Install Apache\n      apt:\n        name: apache2\n        state: present\n \n    - name: Copy the Apache config file\n      template:\n        src: templates/apache2.conf.j2\n        dest: /etc/apache2/apache2.conf\n      notify: \n        - Restart Apache\n \n  handlers:\n    - name: Restart Apache\n      service:\n        name: apache2\n        state: restarted\n為什麼使用 handlers？\n避免重複操作來提高效率和簡化 playbook。如果多個 task 都需要修改某個檔案，你可以讓這些 task 都通知一個 handler，最後只會執行一次重啟操作。\n其他用法\nHandlers 也可以通知其他 handlers，這在需要進行更複雜的操作時非常有用。例如：\nhandlers:\n  - name: Restart Apache\n    service:\n      name: apache2\n      state: restarted\n    notify: \n      - Reload Firewall\n \n  - name: Reload Firewall\n    command: firewall-cmd --reload"},"Operations/Ansible/ansible-learning-01":{"title":"用 ChatGPT 學習 Ansible (1)","links":["Operations/Ansible/ansible-handler"],"tags":["ansible","devops","learn_with_gpt"],"content":"Ansible 檔案架構\n官方 ansible 的檔案架構\n範例檔案架構：\n.\n├── ansible.cfg\n├── inventory\n├── main.yml\n└── roles\n    └── hello_world\n        ├── tasks\n        │   └── main.yml\n        └── vars\n            └── main.yml\n\n在一個典型的 Ansible 專案中，會有以下幾個主要文件和目錄：\n\n\nansible.cfg：這是 Ansible 的配置文件，用於設置一些全域性的參數，比如 inventory 文件的路徑、遠端用戶名、模組路徑等。\n\n\nhosts (或 inventory)：這是定義受管理主機的文件，可以是 .ini 或 .yaml 格式。這個文件告訴 Ansible 要管理哪些主機。\n\n\nplaybooks：這些是 Ansible 的核心文件，用於定義一系列的任務。Playbooks 通常使用 YAML 格式編寫。\n\n\nroles：這是一種結構化的方式來組織 playbooks，讓程式碼更易於重用和維護。Roles 包含了任務、變數、文件、模板、處理程序和其他資源。\n\n\ngroup_vars 和 host_vars：這些目錄和文件用於定義針對特定主機或主機群組的變數。\n\n\n基本名詞解釋\n\n\nPlaybook：定義了 Ansible 任務的 YAML 文件。Playbooks 包含一個或多個 plays，每個 play 定義了要執行的任務集合和目標主機。\n\n\nPlay：Play 是 Playbook 的組成部分，一個 play 定義了一組在特定主機上執行的任務。Play 指定了目標主機和這些主機上要執行的任務。\n\n\nTask：任務是 Ansible 的基本單位，一個 task 通常對應一個模組取用。每個任務都會執行一個模組並指定模組的參數。\n\n\nModule：模組是 Ansible 用來執行實際任務的單元，例如安裝軟體、管理文件、執行命令等。Ansible 有許多內建的模組，也支援第三方模組。\n\n\nRole：Role 是組織和重用 Ansible 內容的一種方式。Roles 可以包含變數、文件、模板、任務和處理程序等。\n\n\nInventory：定義了 Ansible 要管理的主機列表，可以是 .ini 或 .yaml 格式，或動態 inventory。\n\n\nHandler：Handler\n\n\nVariable：變數用於存取和傳遞數據，可以在 Playbooks、Roles、和模組中使用。\n\n\n範例 playbook:\nplaybook.yaml---\n- name: Install and configure web server\n  hosts: webservers\n  become: yes\n \n  tasks:\n    - name: Install Apache\n      apt:\n        name: apache2\n        state: present\n \n    - name: Copy the Apache config file\n      template:\n        src: templates/apache2.conf.j2\n        dest: /etc/apache2/apache2.conf\n      notify:\n        - Restart Apache\n \n  handlers:\n    - name: Restart Apache\n      service:\n        name: apache2\n        state: restarted"},"Operations/Ansible/ansible-terraform-compare":{"title":"Ansible 和 Terraform 的差異","links":[],"tags":["ansible","terraform","devops","iac","learn_with_gpt"],"content":"Terraform 和 Ansible 都是基礎設施即代碼（Infrastructure as Code, IaC）工具，但它們有不同的用途和特性。\nTerraform\n\n用途：主要用於管理基礎設施，如雲資源的佈署、修改和管理。Terraform 支援多種雲平台（如 AWS、Azure、GCP），以及其他服務提供商。\n工作方式：使用宣告式語言（Declarative Language）。你定義想要的最終狀態，Terraform 負責達到這個狀態。例如，你指定要有 3 個虛擬機，Terraform 會確保最終狀態是 3 個虛擬機，無論最初是 0 個還是 2 個。\n狀態管理：Terraform 有一個狀態文件來記錄基礎設施的現狀，確保每次應用變更時都能基於最新的狀態進行計算和更新。\n計劃與執行：Terraform 提供 terraform plan 指令來預覽變更，並用 terraform apply 執行變更，這樣可以在實際改變基礎設施前先檢查變更內容。\n\nAnsible\n\n用途：主要用於配置管理、自動化任務和應用部署。Ansible 可以用來安裝軟體、配置伺服器、管理服務等。\n工作方式：使用宣告式和指令式語言的混合（Ansible Playbooks 是 YAML 格式）。你可以定義具體的操作步驟，例如安裝某個軟體包、編輯配置文件、重啟服務等。\n無狀態管理：Ansible 不保留狀態文件，每次執行 Playbook 都是從頭開始。這意味著 Ansible 每次運行時會重新檢查和執行所有任務。\n推送模式：Ansible 使用 SSH 連接到目標伺服器並執行命令，這種方式稱為推送模式（Push Mode）。與此相對的，Terraform 是拉取模式（Pull Mode），它依賴於 API 與雲提供商互動。\n\n總結\n\n用途不同：Terraform 側重於基礎設施的管理和佈署，Ansible 側重於配置管理和應用部署。\n語言和工作方式不同：Terraform 使用宣告式語言，定義最終狀態；Ansible 使用宣告式和指令式的混合，定義具體操作步驟。\n狀態管理：Terraform 使用狀態文件追蹤資源狀態，而 Ansible 不保留狀態。\n運行模式：Terraform 是拉取模式，Ansible 是推送模式。\n\n這兩種工具可以互補使用，例如先用 Terraform 建立基礎設施，再用 Ansible 配置和管理這些資源。"},"Operations/Ansible/index":{"title":"Ansible","links":[],"tags":[],"content":""},"Operations/Container/index":{"title":"Container","links":[],"tags":[],"content":""},"Operations/Container/kubernetes/index":{"title":"Kubernetes (k8s)","links":[],"tags":[],"content":""},"Operations/Container/kubernetes/k8s-admission-controller":{"title":"Kubernetes Admission Controller","links":["Operations/Container/kubernetes/k8s-resource-quota","Operations/Container/kubernetes/k8s-limit-range"],"tags":["container/kubernetes","k8s-security","learn_with_gpt"],"content":"介紹\nKubernetes Admission Controller 是一段程式碼，在物件（object, 例如 Pod、Service 等）被寫入persistence storage (etcd)，以及 kubernetes request 被驗證 (authenticated) 和授權 (authorized) 之後，攔截對 Kubernetes API 的 request，並對其變更、驗證。\n\nAdmission Controller 主要分為兩種類型：Mutating Admission 和 Validating Admission。\nMutating Admission\n在物件被儲存進 etcd 之前修改其屬性。例如，可以自動新增缺少的欄位或根據某些規則修改物件。\nValidating Admission\n驗證請求的合法性，確保請求符合集群的策略和安全規範。例如，禁止特定的映像或限制資源的使用。\n內建的 Admission Controller\n\nAdmission Controllers Reference | Kubernetes\n\n\nNamespaceLifecycle: 確保資源只能在有效的命名空間中創建\nResourceQuota: 強制執行資源配額的限制\nLimitRanger: 強制執行 Pod 和 Container 的資源限制\nServiceAccount: 自動為 Pod 指派 ServiceAccount\n\n自定義 Admission Webhook\n\n在 Kubernetes 上註冊一個 webhook config\n\n兩種 kind: ValidatingWebhookConfiguration、MutatingWebhookConfiguration\n\n\n起一個 HTTP server 處理 api-server 的內容\n\n\n\n                  \n                  範例 \n                  \n                \n\nValidatingWebhookConfigurationapiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: &quot;pod-policy.example.com&quot;\nwebhooks:\n- name: &quot;pod-policy.example.com&quot;\n  rules:\n  - apiGroups:   [&quot;&quot;] # 攔截資源的 Group：&quot;&quot; 表示 core、&quot;*&quot; 表示所有\n    apiVersions: [&quot;v1&quot;] # 攔截資源的版本號\n    operations:  [&quot;CREATE&quot;] # 攔截什麼樣的請求\n    resources:   [&quot;pods&quot;]  # 攔截的資源\n    scope:       &quot;Namespaced&quot; # 生效的範圍，cluster 或 namespace，&quot;*&quot; 表示沒有範圍限制\n  clientConfig: # 我們部屬的 webhook 服務\n    service: # service 是在 cluster-in 模式\n      namespace: &quot;example-namespace&quot;\n      name: &quot;example-service&quot;\n      port: 443\n      path: &quot;/validate&quot; # path 是對應驗證的端點\n      caBundle: &quot;Ci0tLS0tQk...&lt;base64-encoded PEM bundle containing the CA that signed the webhook&#039;s serving certificate&gt;...tLS0K&quot;\n  admissionReviewVersions: [&quot;v1&quot;, &quot;v1beta1&quot;]\n  sideEffects: None\n  timeoutSeconds: 5 # 1-30s，表示 api 的逾時時間\nMutatingWebhookConfigurationapiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: &quot;valipod-policy.example.com&quot;\nwebhooks:\n- name: &quot;valipod-policy.example.com&quot;\n  rules:\n    - apiGroups:   [&quot;apps&quot;]\n      apiVersions: [&quot;v1&quot;]\n      operations:  [&quot;CREATE&quot;]\n      resources:   [&quot;deployments&quot;]\n      scope:       &quot;Namespaced&quot;\n  clientConfig:\n    url: &quot;https://10.0.0.1:81/validate&quot; # 在 cluster 外部的模式\n    #      service:\n    #        namespace: &quot;default&quot;\n    #        name: &quot;admission-webhook&quot;\n    #        port: 81\n    #        path: &quot;/mutate&quot;\n    caBundle: &quot;Ci0tLS0tQk...&lt;base64-encoded PEM bundle containing the CA that signed the webhook&#039;s serving certificate&gt;...tLS0K&quot;\n  admissionReviewVersions: [&quot;v1&quot;]\n  sideEffects: None\n  timeoutSeconds: 5\nAdmissionReview{\n  &quot;kind&quot;: &quot;AdmissionReview&quot;,\n  &quot;apiVersion&quot;: &quot;admission.k8s.io/v1&quot;,\n  &quot;request&quot;: {\n    &quot;uid&quot;: &quot;705ab4c5-6393-11e8-b7cc-42010a800002&quot;,\n    &quot;kind&quot;: {&quot;group&quot;: &quot;&quot;, &quot;version&quot;: &quot;v1&quot;, &quot;kind&quot;: &quot;Pod&quot;},\n    &quot;resource&quot;: {&quot;group&quot;: &quot;&quot;, &quot;version&quot;: &quot;v1&quot;, &quot;resource&quot;: &quot;pods&quot;},\n    &quot;namespace&quot;: &quot;default&quot;,\n    &quot;operation&quot;: &quot;CREATE&quot;,\n    &quot;userInfo&quot;: {\n      &quot;username&quot;: &quot;admin&quot;,\n      &quot;uid&quot;: &quot;014fbff9a07c&quot;,\n      &quot;groups&quot;: [&quot;system:authenticated&quot;, &quot;my-admin-group&quot;],\n      &quot;extra&quot;: {\n        &quot;some-key&quot;: [&quot;some-value1&quot;, &quot;some-value2&quot;]\n      }\n    },\n    &quot;object&quot;: {\n      &quot;metadata&quot;: {\n        &quot;name&quot;: &quot;my-pod&quot;,\n        &quot;namespace&quot;: &quot;default&quot;,\n        &quot;labels&quot;: {&quot;app&quot;: &quot;my-app&quot;}\n      },\n      &quot;spec&quot;: {\n        &quot;containers&quot;: [\n          {\n            &quot;name&quot;: &quot;my-container&quot;,\n            &quot;image&quot;: &quot;my-image&quot;\n          }\n        ]\n      }\n    },\n    &quot;oldObject&quot;: null,\n    &quot;dryRun&quot;: false\n  }\n}\n\n\n                  \n                  v1.30 ValidatingAdmissionPolicy 和 AdmissionWebhookMatchConditions 進入 GA\n                  \n                \n\n用途\n安全策略的實施\n\n限制特權容器：防止在集群中運行具有特權的容器。可以使用 PodSecurityPolicy（雖然在 Kubernetes 1.25 開始被移除，但可以使用替代的 Open Policy Agent 或 Kyverno 等工具）來限制 Pod 運行特權模式。\n阻止主機網路：阻止 Pod 使用主機網路或主機的 PID/IPC 命名空間，以增強隔離性。\n強制映像簽名驗證：可以實施控制器來檢查容器映像是否有合法的簽名，確保只有信任的映像被部署。\n\n資源配額和限制\n\n配額管理：使用 ResourceQuota 來限制命名空間中可用的資源總量，例如 CPU、記憶體、PersistentVolumeClaim 的數量等。\n資源請求限制：使用 LimitRange 來設置 Pod 或容器的資源請求和限制範圍，避免單個 Pod 消耗過多資源。\n\n合規性與政策強制\n\n標籤和注釋檢查：強制要求 Pod、Service、Deployment 等資源包含特定的標籤或注釋，這在實施組織內部的標準化和合規性策略時非常有用。\n命名規則強制：檢查和強制執行資源名稱的規則，確保名稱符合組織的命名慣例或政策。\n\n網路策略控制\n\n強制網路隔離：確保所有 Pod 必須遵循特定的網路策略，這樣可以防止未經允許的網路通信。\n自動注入網路策略：在資源創建時自動注入預定義的網路策略，確保 Pod 在部署時即具有適當的網路隔離。\n\n動態配置管理\n\n自動注入 sidecar 容器：使用 MutatingAdmissionWebhook 來自動注入 sidecar 容器（如 Istio 的 Envoy 代理或監控代理）到 Pod 中，這樣可以確保所有應用都遵守網路管理或監控要求。\n配置文件注入：動態地將 ConfigMap 或 Secret 注入到容器內，確保配置管理的靈活性和一致性。\n\n策略執行與審計\n\nPod 安全性策略（PodSecurityPolicy）：雖然 PodSecurityPolicy 已被廢棄，但在 Kubernetes 1.25 之前，它被廣泛用於控制 Pod 的安全配置，如允許的卷類型、執行用戶、文件系統權限等。\nPod 安全性標準（PodSecurity Standard）：新的 Pod 安全性標準允許在命名空間級別配置安全級別（如 “restricted”, “baseline”, “privileged”），並通過 Admission Controller 強制執行。\n\n資源自動標籤\n\n環境標籤：自動將資源標記為 development, staging, production 等，基於所部署的環境進行標識。\n審計標籤：自動將創建者或時間戳等信息標記到資源中，便於後續審計和排查。\n\n拒絕不安全或不符合政策的操作\n\n阻止不允許的卷類型：防止 Pod 使用不允許的卷類型（如主機路徑卷），這可能會引發安全風險。\n阻止使用不安全的映像版本：防止部署包含已知漏洞的容器映像。\n\n版本控制\n\n升級策略強制：防止不兼容的應用版本在不正確的 Kubernetes 版本中部署，以避免潛在的運行時問題。\n\n自動化流程\n\n自動填充配置：在資源創建過程中，根據標準模板自動填充必要的配置項目（如環境變數、卷掛載等）。\n自動化測試：在資源創建或更新前，執行自動化測試或校驗腳本，確保資源符合預期。\n\n參考資料\n\nAdmission Controllers Reference | Kubernetes\nA Guide to Kubernetes Admission Controllers | Kubernetes\n理清 Kubernetes 中的准入控制（Admission Controller) | MoeLove\nAdmission controllers: the good, the bad, the ugly | Nic Vermande - YouTube\n"},"Operations/Container/kubernetes/k8s-attack":{"title":"Kubernetes 的攻擊","links":[],"tags":["container/kubernetes","k8s-security"],"content":"\nCVEs\n\nCVE漏洞修複 - Container Service for Kubernetes - 阿里雲\n\nCVE-2024-5321,CVE-2024-3177,CVE-2023-51699,CVE-2024-21626,CVE-2023-2878,CVE-2023-2727,CVE-2023-2728,CVE-2023-30840,CVE-2023-27561,CVE-2023-28642,CVE-2023-25153,CVE-2023-25173,CVE-2022-23471,CVE-2022-3294,CVE-2022-3162,CVE-2022-3172,CVE-2022-31030,CVE-2021-25748,CVE-2021-25746,CVE-2021-25745,CVE-2022-23648,CVE-2022-0492,CVE-2022-0185,CVE-2021-25742,CVE-2021-41103,CVE-2021-25741,CVE-2021-25740,CVE-2021-25738,CVE-2021-25737,CVE-2021-30465,CVE-2021-3121,CVE-2020-8562,CVE-2021-25735,CVE-2021-1056,CVE-2020-8554,CVE-2020-15257,CVE-2018-18264,CVE-2018-10021,CVE-2019-5736,CVE-2019-11246,CVE-2019-11249,CVE-2019-11253,CVE-2019-16276,CVE-2019-10021,CVE-2020-8555,CVE-2020-8558,CVE-2020-13401,CVE-2020-8559,CVE-2020-8557,CVE-2020-14386,CVE-2020-8564,CVE-2020-8565,CVE-2020-8566\n參考資料\n\nDay18 - (攻擊) 介紹攻擊 api server - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天\n\n【云攻防系列】从攻击者视角聊聊K8S集群安全（上） - FreeBuf网络安全行业门户\nTactics - Threat Matrix for Kubernetes\nCVE漏洞修複 - Container Service for Kubernetes - 阿里雲\nArray.from(document.getElementsByClassName(&#039;markdown-body&#039;)[0].childNodes[1].childNodes).map(el =&gt; el.innerText.match(/CVE-\\d{4}-\\d{4,5}/g).join(&#039;,&#039;)).join(&#039;,&#039;)\n\n\nKubernetes Security - OWASP Cheat Sheet Series\n"},"Operations/Container/kubernetes/k8s-hpa":{"title":"Kubernetes Horizontal Pod Autoscaler (HPA)","links":[],"tags":["container/kubernetes","k8s-object"],"content":"參考資料\nKubernetes HPA：自動化應用擴展的藝術. 引言 在當今的技術世界中，Kubernetes… | by Alandev | Medium"},"Operations/Container/kubernetes/k8s-limit-range":{"title":"Kubernetes Limit Range","links":[],"tags":["container/kubernetes","k8s-object"],"content":"可以使用 Limit Range 限制 Pod 或 Container 維度的資源使用\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: compute-resource\n  namespace: departmant-qa\nspec:\n  limits:\n  - default: # 預設限制量\n      cpu: 500m # resource.limits.cpu\n      memory: 500Mi\n    defaultRequest: # 預設取用量\n      cpu: 100m # resource.requests.cpu\n      memory: 10Mi\n    max: # 最大/低使用量\n      cpu: &quot;2&quot;\n      memory: 4Gi\n    min:\n      cpu: 100m\n      memory: 10Mi\n    type: Container\n參考資料\n\nLimit Ranges | Kubernetes\n"},"Operations/Container/kubernetes/k8s-multi-tenancy":{"title":"Kubernetes Multi Tenancy","links":[],"tags":["container/kubernetes"],"content":"參考資料\nMulti-Tenancy Kubernetes Cluster Part 1: 認命吧！有一好，就沒兩好！ | by smalltown | Starbugs Weekly 星巴哥技術專欄 | Medium"},"Operations/Container/kubernetes/k8s-networking":{"title":"Kubernetes 的網路世界 - 文章蒐集區","links":["Operations/Container/kubernetes/k8s-service"],"tags":["container/kubernetes","k8s-networking"],"content":"\nKubernetes Service\nThe Service Mesh Manifesto\n\nOverview | Linkerd\n\nEnvoy proxy - home\n\n\n\n\n\n參考資料\n\nKubernetes 之 Service 與 kube-proxy 深入理解 | 歐里丸Blog\n"},"Operations/Container/kubernetes/k8s-object":{"title":"Kubernetes Objects (元件)","links":["Operations/Container/kubernetes/k8s-service","Operations/Container/kubernetes/k8s-resource-quota","Operations/Container/kubernetes/k8s-limit-range"],"tags":["container/kubernetes","k8s-object"],"content":"\n節錄：Objects In Kubernetes | Kubernetes\n\n前言\n可以使用 .yaml 來描述一個 kubernetes 元件\nfields\n\napiVersion：元件的版本號，可以從 kubectl api-versions 取得支援的版號1。\nkind：元件是什麼屬性，常見有 Pod、Node、Service、Namespace、ReplicationController。\nmetadata\n\nname 指定該元件的名稱\nlabels 指定該元件的標籤，用於定位\n\n\nspec：What state you desire for the object, a description of a object\n\nResource Object Types\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategoryResource Object Type NameWorkload2Pod, HorizontalPodAutoscaler(HPA)ControllerDeployment, ReplicaSet, ReplicationController, StatefulSet, DaemonSet, Job, CronJobService DiscoveryService, IngressAuthentication &amp; AuthorizationServiceAccount, RBAC(Role, ClusterRole, RoleBinding, ClusterRoleBinding)StorageVolume, PersistentVolume, PersistentVolumeClaim, StorageClass, Secret, ConfigMapPolicyNetworkPolicy, SecurityContext, ResourceQuota, LimitRangeExtensionCustomResourceDefinitions\n參考資料\n\n[Kubernetes] Resource Object 概觀 | 小信豬的原始部落\n\nFootnotes\n\n\nKubernetes 升級 FAQ. Kubernetes 升級速度滿快的，至少一年會需要更新一次大版，遇到 API… | by smalltown | Starbugs Weekly 星巴哥技術專欄 | Medium ↩\n\n\nWorkload Resources | Kubernetes ↩\n\n\n"},"Operations/Container/kubernetes/k8s-resource-management":{"title":"Kubernetes 的資源管理","links":["Operations/Container/kubernetes/k8s-resource-quota","Operations/Container/kubernetes/k8s-limit-range"],"tags":["container/kubernetes"],"content":"可以使用 Resource Quota 限制 namespace 的資源使用，但仍有可能 被單一或少數 Pod 一次耗盡，故可以使用 Limit Range 來限制 Pod 或 Container 維度的資源要求範圍。"},"Operations/Container/kubernetes/k8s-resource-quota":{"title":"Kubernetes Resource Quota","links":[],"tags":["container/kubernetes","k8s-object"],"content":"Resource Quota 可以限制 namespace 下的資源使用，如 CPU、記憶體、儲存空間、甚至是 object 數量等。\n設定後， namespace 下的所有資源都強制要寫明資源使用量才可以被部屬。"},"Operations/Container/kubernetes/k8s-service":{"title":"Kubernetes Service","links":["Operations/Container/kubernetes/k8s-ingress"],"tags":["container/kubernetes","k8s-object","k8s-networking"],"content":"\n節錄自：Service | Kubernetes\n\n\nService 是 k8s 中負責管理東西向流量，南北向流量可以參考 Ingress\n路由設定是透過 Selector 與 Label 機制實現\nService 的種類 (Type)：NodePort , LoadBalancer, ClusterIP, ExternalName\n\nNodePort 可以將 Service 暴露給外網 (外對內)\nClusterIP 用於 Service 對內部容器的反向代理 (內對內)\nLoadBalancer (外對內)\nExternalName (內對外)\n\n\n\n其他參考資料\n\n[Kubernetes / K8s]K8s的反向代理 -Service 與實作發布Services || Kubernetes Service | by KouWei.Lee | k8s筆記 | Medium\nKubernetes 基礎教學（二）實作範例：Pod、Service、Deployment、Ingress | Cheng-Wei Hu\n"},"Operations/Container/kubernetes/k8s":{"title":"Kubernetes 資源集散地","links":["Operations/Container/kubernetes/k8s-object","Operations/Container/kubernetes/k8s-resource-management","Operations/Container/kubernetes/k8s-networking"],"tags":["container/kubernetes"],"content":"資源區\n\nk8s-object\n\n主題區\n\nk8s-resource-management\nk8s-networking\n\n參考資料\n\nCommunity Icons\nKubernetes 基礎教學（一）原理介紹 | Cheng-Wei Hu\nDay 30 更多疑問 x 關鍵字 x 心得 - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天\nKubernetes Roadmap. Kubernetes已迅速成為容器編排和管理的事實標準。隨著越來越多的組織採用K… | by Moris | Linux &amp; Backend Notes | Medium\n"},"Operations/Container/talos/index":{"title":"Talos","links":[],"tags":[],"content":""},"Operations/Container/talos/talos-commands":{"title":"Talos 常用的指令","links":[],"tags":["talos"],"content":"config file\ntalos config file 存在 ~/.talos/config\ndashboard\n類似遠端 ssh 進入機器的指令\n$ talosctl dashboard -n 10.2.7.11\n我看過的教學文\n\nTalos Linux with Cilium - HackMD\nCLI | Talos Linux\nTalos linux 在沒有憑證的環境之下如何取得網卡資訊 - HackMD\n"},"Operations/Container/talos/talos-config-patch":{"title":"Talos 修改機器設定 - Configuration Patches","links":[],"tags":["talos"],"content":"\n節錄：Configuration Patches | Talos Linux\n"},"Operations/Container/talos/talos-extension":{"title":"Talos 安裝 Extension","links":["Operations/Container/talos/talos-upgrade"],"tags":["talos"],"content":"\n節錄：System Extensions | Talos Linux 以及 Image Factory | Talos Linux\n\n簡介\n由於 Talos 是一個 Immutable OS，root filesystem 是唯讀的，透過 System Extension 可以幫助我們在系統上安裝 container runtimes、firmware、driver 等等的東西。\nSystem Extension 只會在系統安裝或升級時被觸發、啟動，執行後 Talos root filesystem 會保持 immutable 和 readonly。\n官方 Extensions：GitHub - siderolabs/extensions: Talos Linux System Extensions\n一些名詞\nImage Factory\n\nImage Factory generates customized Talos Linux images based on configured schematics.\n\nSchematics\n\nSchematics are YAML files that define customizations to be applied to a Talos Linux image. Schematics can be applied to any of the versions of Talos Linux offered by the Image Factory to produce a “model”, which is a Talos Linux image with the customizations applied.\nSchematics are content-addressable, that is, the content of the schematic is used to generate a unique ID. The schematic should be uploaded to the Image Factory first, and then the ID can be used to reference the schematic in a model.\n\n安裝方式\n可以利用 Image Factory 的方式安裝系統 Extension：\n方法一：圖形化介面\n\n進入 Image Factory UI\n按照步驟完成所有選項\n網頁會生成一組 schematic ID、YAML 格式的 Schematics、和一組 ISO 下載點\n\n若是初始安裝，可以直接拿 ISO 檔安裝\nSchematics 可以拿來參考？\n若已經安裝好，可以使用 schematic ID 執行 upgrade\n\ntalos upgrade --image factory.talos.dev/installer/&lt;id&gt;:v&lt;talos_version&gt; -m powercycle -f\n\n\n若是初始安裝但下載好原版 ISO，可以將 schematic ID 寫入 Machine Config\n\n\n\nmachine:\n  install:\n    image: factory.talos.dev/installer/&lt;id&gt;:v&lt;talos_version&gt;\t\n方法二：指令介面\n\n撰寫 Schematics，其中 officialExtensions 可以參考官方 Extensions 清單\n\ntalos-extension-proxmox.yamlcustomization:\n  systemExtensions:\n    officialExtensions:\n      - siderolabs/qemu-guest-agent\n\n取得 schematics id\n\n$ curl -X POST --data-binary @talos-extension-proxmox.yaml factory.talos.dev/schematics\n# {&quot;id&quot;:&quot;f683308f6e49ec36d715bd41c90d800910552d8cbc015c8363ec350594535fa1&quot;}\n\n執行 upgrade 或寫入 machine config\n\n$ talos upgrade --image factory.talos.dev/installer/&lt;id&gt;:v&lt;talos_version&gt; -m powercycle -f\n查看安裝的 Extensions\n可以使用 talosctl get extensions 列出目前機器上所安裝的 extensions\n$ talosctl get extensions\n# NODE         NAMESPACE   TYPE              ID                                              VERSION   NAME          VERSION\n# 172.20.0.2   runtime     ExtensionStatus   000.ghcr.io-talos-systems-gvisor-54b831d        1         gvisor        20220117.0-v1.0.0\n# 172.20.0.2   runtime     ExtensionStatus   001.ghcr.io-talos-systems-intel-ucode-54b831d   1         intel-ucode   microcode-20210608-v1.0.0\n或是 talosctl service &lt;service_name&gt; 查看某一個元件的運作狀況\n$ talosctl service ext-qemu-guest-agent\n# NODE     192.168.1.1\n# ID       ext-qemu-guest-agent\n# STATE    Running\n# HEALTH   ?\n# EVENTS   [Running]: Started task ext-qemu-guest-agent (PID 1941) for container ext-qemu-guest-agent (4s ago)\n#          [Preparing]: Creating service runner (4s ago)\n#          [Preparing]: Running pre state (4s ago)\n#          [Waiting]: Waiting for service &quot;cri&quot; to be &quot;up&quot; (5s ago)\n#          [Waiting]: Waiting for service &quot;containerd&quot; to be &quot;up&quot;, service &quot;cri&quot; to be registered, file &quot;/dev/virtio-ports/org.qemu.guest_agent.0&quot; to exist (6s ago)\n#          [Waiting]: Waiting for service &quot;containerd&quot; to be registered, service &quot;cri&quot; to be registered, file &quot;/dev/virtio-ports/org.qemu.guest_agent.0&quot; to exist (8s ago)\n#          [Waiting]: Waiting for service &quot;containerd&quot; to be &quot;up&quot;, service &quot;cri&quot; to be &quot;up&quot;, file &quot;/dev/virtio-ports/org.qemu.guest_agent.0&quot; to exist (9s ago)\n其他參考資料\n\nHow to install Tailscale on Talos Linux - YouTube\n"},"Operations/Container/talos/talos-interface-name":{"title":"Talos 網卡的命名","links":[],"tags":["talos"],"content":"\n節錄：Predictable Interface Names | Talos Linux\n\n簡介\n有別於傳統的網卡名稱 eth0、eth1，Talos 在 v1.5 後使用 predictable names 的方式命名網卡，如同 systemd 的規則。\n為什麼要這樣做？\neth0、eth1 這種命名方式是根據驅動 (driver) 決定的，但這個機制可能會因為驅動初始化順序不同導致介面順序不固定，也就是說 eth0 在下一次開機時可能會變成 eth1。在這個情況下，使用網卡名稱的元件如防火牆，可能會因此發生問題。為了解決這種不可預測性，systemd v197 開始使用新命名方式。\n規則是什麼？\n依照網卡特性命名，Talos 預設使用 MAC Address，如 enx78e7d1ea46da\n\nfirmware/BIOS 對板載裝置的 index，如 eno1\nfirmware/BIOS 對熱插拔 PCIe 裝置的 index，如 ens1\n硬體連接器，如 PCIe 上的網卡，可能有多個插孔，所以會出現 s0、s1 的編號，如 enp2s0\n\n上述都是 en 開頭的網卡，其實還有其他的，如\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrefixDescriptionenEthernetibInfiniBandslSerial line IP (slip)wlWireless local area network (WLAN)wwWireless wide area network (WWAN)\n不喜歡怎麼辦？\n改回 eth0 的命名方式\n雖然建議使用新式命名，但想改回 eth0 的話，可以在 kernel argument 上新增 net.ifnames=0\nmachine:\n  install:\n    extraKernelArgs:\n      - net.ifnames=0\n使用 device selector\n可以使用官方的 device selector 語法選擇網卡\nmachine:\n  network:\n    interfaces:\n      - deviceSelector:\n          busPath: &quot;0*&quot; # 這行代表選中所有硬體網卡，當該環境中只有一張網卡時，此方法很實用\n其他參考資料\n\nDebian 9 更改網路介面預設命名方式 – SZ Lin with Cybersecurity &amp; Embedded Linux\nsystemd.net-naming-scheme\n"},"Operations/Container/talos/talos-machineconfig-patch":{"title":"Talos 事後修改機器設定 - Machine Configuration Patch","links":[],"tags":["talos"],"content":"\n節錄：Editing Machine Configuration | Talos Linux\n"},"Operations/Container/talos/talos-reset":{"title":"Talos 重設節點或叢集","links":[],"tags":["container/kubernetes","talos"],"content":"\n節錄： Resetting a Machine | Talos Linux\n\n簡介\ntalosctl reset 指令是用來重設整台/整個機器，重設後就像是全新電腦一樣，請小心使用，尤其在雲端 VM 環境\n指令\n$ talosctl reset --talosconfig=./clusterconfig/talosconfig --nodes=10.2.7.11 --graceful=false --reboot;\n\n若無指定 --nodes，會直接重設 talosconfig 中的所有機器\n重設完成後，無指定 --reboot 會保持機器關閉狀態\nESXi VM 重設後會處於無作業系統狀態，要以 ISO 檔案開機\n\n後記\n截至 v1.7.3，我發現使用 reset 指令並沒有真正完全清空機器，都是進 ISO 的 reset 才會正常，不知道自己哪裡做錯。"},"Operations/Container/talos/talos-setup-01":{"title":"Talos 安裝紀錄 (1) - 簡介與環境","links":["Operations/Container/talos/talos-setup","Operations/Container/talos/talos-setup-02"],"tags":["talos","container/kubernetes","homelab"],"content":"簡介\n大概在 2015，我剛上大學的時候，容器化的概念開始崛起，什麼設定都要手動來，裝 kubernetes 得撞好多 dependency。到了 2017，學會使用 cloudera、rancher 來裝 k8s cluster。而 2024 的現在，建立 kubernetes 毫無難度，一鍵就能安裝完成還不會出錯，甚至出現各種「內建 kubernetes 的作業系統」，而這篇文想使用的 Talos 就是其中一種。\nTalos 是一個很有趣的作業系統，他有幾個特性：\n\nContainer OS：內建 kubernetes\nMinimal：作業系統的 ISO 檔不到 100M！除了主要作業系統、kubernetes，沒有其他多餘的工具，資源佔用低\nSecure：精簡的作業系統，攻擊面小\nImmutable OS：root filesystem 唯讀不可變，無法自行寫入和修改，保證系統完整性\n一個設定檔 (declarative configuration file) 搞定一台機器\n沒有 ssh 登入，全使用 gRPC API talosctl 來控制機器，就如使用 kubectl 控制 kubernetes 叢集一樣的概念\n\n當然，特殊用途的作業系統也有些不方便的地方：\n\n沒有像是 apt 或 yum 的 package 管理工具。不過官方有 repo 可以安裝，如 iSCSI 驅動、qemu guest 等等\n學習成本高，不是大眾熟悉的作業系統，要花時間學寫設定檔\n\n這是一個系列文，目錄 在此。\n實驗環境\n\n\n                  \n                  示範環境是 VM，在 Proxmox 8.2、ESXi 6.7 上，作業系統為 macOS，使用 talos 1.7 \n                  \n                \n\n節點列表\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameIPRoleHypervisorcontrol-e0110.2.7.11/23Control PlaneProxmoxcontrol-e0210.2.7.12/23Control PlaneESXicontrol-e0310.2.7.13/23Control PlaneProxmox---10.2.7.15/23Control Plane (VIP)---worker-e0110.2.7.16/23Worker NodeProxmoxworker-e0210.2.7.17/23Worker NodeESXi\n機器規格\n\n節錄：System Requirements | Talos Linux\n\n最低需求（Minimum）：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoleMemoryCoresSystem DiskControl Plane2 GiB210 GiBWorker1 GiB110 GiB\n建議規格（Recommended）：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoleMemoryCoresSystem DiskControl Plane4 GiB4100 GiBWorker2 GiB2100 GiB\n安裝 talosctl\n$ curl -sL talos.dev/install | sh\n# macOS 亦可使用 homebrew 安裝\n$ brew install siderolabs/tap/talosctl\n安裝 kubectl\n\n可參考 kubernetes 官方教學文\n\n\n下一步，我們來撰寫安裝用設定檔：\n我的 Talos 安裝紀錄 (2) - 撰寫設定檔"},"Operations/Container/talos/talos-setup-02":{"title":"Talos 安裝紀錄 (2) - 撰寫 Machine Config","links":["Operations/Container/talos/talos-setup-01","Operations/Container/talos/talos-setup","Operations/Container/talos/talos-extension","Operations/Container/talos/talos-interface-name","Operations/Container/talos/talos-vip","Operations/Container/talos/talos-setup-03","Utilities/soap-age-encryption"],"tags":["talos","container/kubernetes","homelab"],"content":"前言\n在 前篇 已經介紹了 Talos 是什麼，以及要安裝的環境簡介，接下來要撰寫安裝用的設定檔。\n這是一個系列文，目錄 在此。\n產生安裝用設定檔 (Machine Config)\ntalos 與普通安裝作業系統不太一樣，不是裝好系統後才下指令設定環境，而是安裝前就撰寫好設定檔 (Machine Config)，系統安裝時會照著那份 spec 設置機器，內建可以做到俗稱的 IaC (基礎設施即程式碼，Infrastructure as Code)。\n首先，我們會先純使用官方的 talosctl，再來介紹第三方工具 talhelper（已棄用）。\n官方作法\n\n先使用 talosctl gen secrets -o secrets.yaml，產生 secrets.yaml，包含了 kubernetes 叢集的溝通用密鑰\n\n$ talosctl gen secrets -o secrets.yaml\n$ cat secrets.yaml\n# cluster:\n#     id: buUIh....\n#     secret: khlN....\n# secrets:\n#     bootstraptoken: qwt....\n#     secretboxencryptionsecret: 3kGCa....\n# trustdinfo:\n#     token: 5kyafg.ce2cod17...\n# certs:\n# ...\n\n同資料夾下，再建立一個檔案 talos-template.yaml，作為叢集的共用設定。全部的設定可以參考官方文件：Config | Talos Linux\n\ntalos-template.yamlmachine:\n  time:\n    servers:\n      - 10.2.6.1 # NTP server on OPNsense\n      - 211.22.103.157 # tock.stdtime.gov.tw\n      - 118.163.81.63 # watch.stdtime.gov.tw\n      - 118.163.81.61 # time.stdtime.gov.tw\n      - 211.22.103.158 # clock.stdtime.gov.tw\n      - 118.163.81.62 # tick.stdtime.gov.tw\n      - /dev/ptp0 # PTP device\n  network:\n    nameservers:\n      - 10.2.6.5\n  install:\n    wipe: true\n \ncluster:\n  network:\n    dnsDomain: efficient.k8s.lab.yee\n    podSubnets:\n      - 10.244.0.0/16\n    serviceSubnets:\n      - 10.96.0.0/12\n\tcni:\n      name: none\n  proxy:\n    disabled: true\n  allowSchedulingOnControlPlanes: true\n\n行 24-27：因想使用 cilium 當作 CNI，所以設定為 none\n\n\n產生帶有 VM Guest Tool 和 UPS NUT Client 的 Extension ID，詳情介紹可參考 Talos - 安裝 Extension\n\ntalos-extension-proxmox.yamlcustomization:\n  systemExtensions:\n    officialExtensions:\n      - siderolabs/qemu-guest-agent\n      - siderolabs/nut-client\ntalos-extension-vmware.yamlcustomization:\n  systemExtensions:\n    officialExtensions:\n      - siderolabs/vmtoolsd-guest-agent\n      - siderolabs/nut-client\n$ curl -X POST --data-binary @talos-extension-proxmox.yaml factory.talos.dev/schematics\n# {&quot;id&quot;:&quot;54524b221db0eeee2d8bb3d21379e04c662c386ecef3745c24a0aa6f51ab31dd&quot;}\n$ curl -X POST --data-binary @talos-extension-vmware.yaml factory.talos.dev/schematics\n# {&quot;id&quot;:&quot;091c828af58eea73d872919931153ffda36cf8ebcdc5b027889412028e7531d3&quot;}\n\n建立每一台機器的專屬設定，這裡分別示範一台 controlplane 和 worker\n\ncontrolplane.11.patch.yamlmachine:\n  network:\n    hostname: control-e01\n    interfaces:\n    - deviceSelector:\n        busPath: &quot;0*&quot;\n      dhcp: false\n      addresses:\n        - 10.2.7.11/23\n      vip:\n        ip: 10.2.7.15\n      routes:\n        - network: 0.0.0.0/0\n          gateway: 10.2.6.1\n      mtu: 8192\n  install:\n      image: factory.talos.dev/installer/54524b221db0eeee2d8bb3d21379e04c662c386ecef3745c24a0aa6f51ab31dd:v1.7.3\n\n行 5-6：參照 talos 的網卡命名\n行 10-11：參照 Talos - Virtual (shared) IP\n行 17：參照 Talos - 安裝 Extension\n\nworker.16.patch.yamlmachine:\n  network:\n    hostname: worker-e01\n    interfaces:\n    - deviceSelector:\n        busPath: &quot;0*&quot;\n      dhcp: false\n      addresses:\n        - 10.2.7.16/23\n      routes:\n        - network: 0.0.0.0/0\n          gateway: 10.2.6.1\n      mtu: 8192\n  install:\n      image: factory.talos.dev/installer/54524b221db0eeee2d8bb3d21379e04c662c386ecef3745c24a0aa6f51ab31dd:v1.7.3\n\n使用 talosctl gen config 產生一台 controlplane 和 worker node 的 Machine Configuration\n\n$ talosctl gen config efficient-cluster01 https://10.2.7.15:6443 --config-patch @talos-template.yaml --with-secrets secrets.yaml --output efficient-cluster01\n \n# generating PKI and tokens\n# Created efficient-cluster01/controlplane.yaml\n# Created efficient-cluster01/worker.yaml\n# Created efficient-cluster01/talosconfig\n\n依序使用 talosctl machineconfig patch 為每台機器產生專屬 Machine Config\n\n$ talosctl machineconfig patch efficient-cluster01/controlplane.yaml --patch @controlplane.11.patch.yaml --output efficient-cluster01/controlplane.11.yaml\n$ talosctl machineconfig patch efficient-cluster01/controlplane.yaml --patch @controlplane.12.patch.yaml --output efficient-cluster01/controlplane.12.yaml\n$ talosctl machineconfig patch efficient-cluster01/controlplane.yaml --patch @controlplane.13.patch.yaml --output efficient-cluster01/controlplane.13.yaml\n$ talosctl machineconfig patch efficient-cluster01/worker.yaml --patch @worker.16.patch.yaml --output efficient-cluster01/worker.16.yaml\n$ talosctl machineconfig patch efficient-cluster01/worker.yaml --patch @worker.17.patch.yaml --output efficient-cluster01/worker.17.yaml\n\n(Optional) 驗證修改後的設定檔\n\n$ talosctl validate --config efficient-cluster01/controlplane.11.yaml --mode metal\n# efficient-cluster01/controlplane.11.yaml is valid for metal mode\n$ talosctl validate --config efficient-cluster01/controlplane.12.yaml --mode metal\n# efficient-cluster01/controlplane.12.yaml is valid for metal mode\n$ talosctl validate --config efficient-cluster01/controlplane.13.yaml --mode metal\n# efficient-cluster01/controlplane.13.yaml is valid for metal mode\n$ talosctl validate --config efficient-cluster01/worker.16.yaml --mode metal\n# efficient-cluster01/worker.16.yaml is valid for metal mode\n$ talosctl validate --config efficient-cluster01/worker.17.yaml --mode metal\n# efficient-cluster01/worker.17.yaml is valid for metal mode\n\n下一步，我們來安裝系統：\n我的 Talos 安裝紀錄 (3) - 安裝系統\n補充：使用 talhelper\n（筆者已棄用此方式，以下流程不再更新，僅供參考）\n從官方作法可以發現，talosctl 沒辦法一次為每一台機器產生專屬的 machine config，導致過程比較繁瑣。\n使用 talhelper 可以幫我們解決這個困擾。\n\n首先，我們先安裝 talhelper\n\n$ brew install talhelper\n\n使用 gensecret 產生 secret file\n\n$ talhelper gensecret &gt; talsecret.yaml\n(Optional) 可以使用 sops 和 age 來加密此 yaml file，就可以安全的讓設定檔上版控\n# 先安裝 sops 和 age\n$ brew install sops age\n \n# 產生 key\n$ age-keygen -o age.key\n# Public key: age1nxlyszwmfuxmplkjkwq3tw459pfpcy6gg6zddj333exetgw5lf5q7f9mzv\n \n$ talhelper gensecret &gt; talsecret.sops.yaml\n$ sops --encrypt -i --age $(cat age.key | sed -n &#039;s/^# public key: \\(.*\\)/\\1/p&#039;) talsecret.sops.yaml\n\n撰寫 talconfig.yaml，詳細說明可參考官網：Configuration - Talhelper\n\ntalconfig.yamlclusterName: efficient-cluster01\nendpoint: https://10.2.7.15:6443\nallowSchedulingOnMasters: true\ndomain: efficient.k8s.lab.yee\ncniConfig:\n  name: none\nnodes:\n  - hostname: control-e01\n    ipAddress: 10.2.7.11\n    controlPlane: true\n    installDisk: /dev/sda\n    nameservers:\n      - 10.2.6.5\n    networkInterfaces:\n      - deviceSelector:\n          busPath: &quot;0*&quot;\n        addresses:\n          - 10.2.7.11/23\n        routes:\n          - network: 0.0.0.0/0\n            gateway: 10.2.6.1\n        vip:\n          ip: 10.2.7.15\n  - hostname: control-e02\n    ipAddress: 10.2.7.12\n    controlPlane: true\n    installDisk: /dev/sda\n    nameservers:\n      - 10.2.6.5\n    networkInterfaces:\n      - deviceSelector:\n          busPath: &quot;0*&quot;\n        addresses:\n          - 10.2.7.12/23\n        routes:\n          - network: 0.0.0.0/0\n            gateway: 10.2.6.1\n        vip:\n          ip: 10.2.7.15\n  - hostname: control-e03\n    ipAddress: 10.2.7.13\n    controlPlane: true\n    installDisk: /dev/sda\n    nameservers:\n      - 10.2.6.5\n    networkInterfaces:\n      - deviceSelector:\n          busPath: &quot;0*&quot;\n        addresses:\n          - 10.2.7.13/23\n        routes:\n          - network: 0.0.0.0/0\n            gateway: 10.2.6.1\n        vip:\n          ip: 10.2.7.15\n  - hostname: worker-e01\n    ipAddress: 10.2.7.16\n    controlPlane: false\n    installDisk: /dev/sda\n    nameservers:\n      - 10.2.6.5\n    networkInterfaces:\n      - deviceSelector:\n          busPath: &quot;0*&quot;\n        addresses:\n          - 10.2.7.16/23\n        routes:\n          - network: 0.0.0.0/0\n            gateway: 10.2.6.1\n  - hostname: worker-e02\n    ipAddress: 10.2.7.17\n    controlPlane: false\n    installDisk: /dev/sda\n    nameservers:\n      - 10.2.6.5\n    networkInterfaces:\n      - deviceSelector:\n          busPath: &quot;0*&quot;\n        addresses:\n          - 10.2.7.17/23\n        routes:\n          - network: 0.0.0.0/0\n            gateway: 10.2.6.1\n \ncontrolPlane:\n  schematic:\n    customization:\n      systemExtensions:\n        officialExtensions:\n          - siderolabs/qemu-guest-agent\nworker:\n  schematic:\n    customization:\n      systemExtensions:\n       officialExtensions:\n         - siderolabs/qemu-guest-agent\n\n產生機器的 Machine Config\n\n# 產生 Machine Config\n$ talhelper genconfig -c talconfig.yaml -s talsecret.yaml -o efficient-cluster01"},"Operations/Container/talos/talos-setup-03":{"title":"Talos 安裝紀錄 (3) - 安裝系統","links":["Operations/Container/talos/talos-setup-02","Operations/Container/talos/talos-setup","Operations/Container/talos/talos-extension"],"tags":["talos","container/kubernetes","homelab"],"content":"前言\n在 前篇 已經撰寫完安裝用的設定檔，接下來就是安裝系統了。\n這是一個系列文，目錄 在此。\n下載相對應的安裝檔案\n如果是 Proxmox 或是實體機器，可以參考 Talos - 安裝 Extension，進 talos image factory 網站下載包含需要的 Extension 的 metal 版 ISO 檔。\nProxmox 記得多勾 qemu guest 的 Extension。\n若是 ESXi，可以直接下載 ova 格式：github.com/siderolabs/talos/releases/download/v1.7.2/vmware-amd64.ova\n建立 VM\nProxmox\n建立 Proxmox VM 時要注意 CPU 選擇 x86-64-v2，在 Proxmox 8.0 以前預設的 kvm64 是不支援的。\n其他設定可以依照自己需求，能掛上 ISO 開機即可。詳情可參考 官方文件\nESXi\nESXi 可使用 ISO 開機，後續的作法如同 Proxmox\n也可使用獨有的方式，以 ova 檔案建立 VM，建立後調整硬碟大小至適當大小，並加上以下設定\nguestinfo.talos.config=$(base64 -i controlplane.yaml)\ndisk.enableUUID=1\n\n圖形化介面操作大概是：\n第一步選擇 OVF File，依照步驟建立 VM\n\n建立完 VM 後，將硬碟、CPU、RAM 調整至適當的大小\n\n進入進階設定\n\n新增兩個鍵值，一個是 guestinfo.talos.config，另一個是 disk.enableUUID\n\nguestinfo.talos.config 的值需要將對應的 controlplane.yaml 轉成 base64 後貼上\ndisk.enableUUID 的值設為 1\n\n\nVM 開機\n避免出錯，我有先將所有 VM 產生的 MAC Address 填入 DHCP Server 綁定 IP\n\nProxomx\n此時的 Proxmox 應該會是 Maintenance Mode，等待我們下指令送出 Machine Configuration\n\n執行 talosctl apply-config 送出指令\n$ talosctl apply-config --talosconfig=./efficient-cluster01/talosconfig --nodes=10.2.7.11 --file=./efficient-cluster01/controlplane.11.yaml --insecure;\n$ talosctl apply-config --talosconfig=./efficient-cluster01/talosconfig --nodes=10.2.7.13 --file=./efficient-cluster01/controlplane.13.yaml --insecure;\n$ talosctl apply-config --talosconfig=./efficient-cluster01/talosconfig --nodes=10.2.7.16 --file=./efficient-cluster01/worker.16.yaml --insecure;\n下完 talosctl apply-config 後，會開始進入 Installing Mode，並重新開機\n\n重新開機後，進入 Booting Mode，kubelet 會從 unhealthy 變成 healthy\n\n\nESXi\n因為前面已經先用 guestinfo.talos.config 餵給 ESXi VM machine config 了，會直接進入 Booting Mode，不需要再下指令\n但若使用 ISO 開機，就要像 Proxmox 的步驟下 talosctl apply-config\n\n設定 talosctl\n在啟動機器之前可以先設定 talosctl，往後下指令就不需要那麼繁瑣，每一次都要加上 --talosconfig 指定設定檔位置\n# controlplanes\n$ talosctl --talosconfig=./efficient-cluster01/talosconfig config endpoint 10.2.7.11 10.2.7.12 10.2.7.13\n# all nodes (controlplane + worker)\n$ talosctl --talosconfig=./efficient-cluster01/talosconfig config node 10.2.7.11 10.2.7.12 10.2.7.13 10.2.7.16 10.2.7.17\n$ talosctl config merge ./efficient-cluster01/talosconfig\n# Config 會被 merge 至 ~/.talos/config\n \n# verify\n$ talosctl config contexts\n# CURRENT   NAME                  ENDPOINTS                       NODES\n# *         efficient-cluster01   10.2.7.11,10.2.7.12,10.2.7.13   10.2.7.11,10.2.7.12,10.2.7.13...\n$ talosctl get members -n 10.2.7.11\n# NODE        NAMESPACE   TYPE     ID            VERSION   HOSTNAME      MACHINE TYPE   OS               ADDRESSES\n# 10.2.7.11   cluster     Member   control-e01   20        control-e01   controlplane   Talos (v1.7.3)   [&quot;10.2.7.11&quot;]\n# 10.2.7.11   cluster     Member   control-e02   12        control-e02   controlplane   Talos (v1.7.3)   [&quot;10.2.7.12&quot;]\n# 10.2.7.11   cluster     Member   control-e03   18        control-e03   controlplane   Talos (v1.7.3)   [&quot;10.2.7.13&quot;]\n# 10.2.7.11   cluster     Member   worker-e01    17        worker-e01    worker         Talos (v1.7.3)   [&quot;10.2.7.16&quot;]\n# 10.2.7.11   cluster     Member   worker-e02    16        worker-e02    worker         Talos (v1.7.3)   [&quot;10.2.7.17&quot;]\n啟動\n進入 Booting Mode 後，等待 control plane 的 kubelet 進入 healthy 狀態，就可以開始啟用叢集了\n啟用叢集只需要對其中一台 control plane 下指令即可\n$ talosctl bootstrap -n 10.2.7.11\n此時會發現 stage 會變成 running，但 ready 會是 false，因為還沒有加上 CNI\n\n下載 kubeconfig\n使用 talosctl kubeconfig 可以協助我們下載 kubeconfig，這樣就可以使用 kubectl 來操作叢集了\n$ talosctl kubeconfig --nodes 10.2.7.11\n# ~/.kube/config 會被 update 叢集\n$ kubectl get nodes\n# NAME          STATUS     ROLES           AGE    VERSION\n# control-e01   NotReady   control-plane   117s   v1.30.1\n# control-e02   NotReady   control-plane   2m4s   v1.30.1\n# control-e03   NotReady   control-plane   2m     v1.30.1\n# worker-e01    NotReady   &lt;none&gt;          98s    v1.30.1\n# worker-e02    NotReady   &lt;none&gt;          116s   v1.30.1\n安裝 CNI\n由上面 kubectl get nodes 的結果可以看到，叢集還是 NotReady 的狀態，因為我們還沒有安裝 CNI\n$ helm repo add cilium helm.cilium.io/\n# &quot;cilium&quot; has been added to your repositories\n$ helm repo update\n# Hang tight while we grab the latest from your chart repositories...\n# ...Successfully got an update from the &quot;cilium&quot; chart repository\n# Update Complete. ⎈Happy Helming!⎈\n$ helm install \\\n    cilium \\\n    cilium/cilium \\\n    --version 1.15 \\\n    --namespace kube-system \\\n    --set ipam.mode=kubernetes \\\n    --set=kubeProxyReplacement=true \\\n    --set=securityContext.capabilities.ciliumAgent=&quot;{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}&quot; \\\n    --set=securityContext.capabilities.cleanCiliumState=&quot;{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}&quot; \\\n    --set=cgroup.autoMount.enabled=false \\\n    --set=cgroup.hostRoot=/sys/fs/cgroup \\\n    --set=k8sServiceHost=localhost \\\n    --set=k8sServicePort=7445 \\\n    --set=hubble.listenAddress=&quot;:4244&quot; \\ # hubble ui 是 cilium 內建的監控工具，可以觀察網路流量\n    --set=hubble.relay.enabled=true \\\n    --set=hubble.ui.enabled=true\n# NAME: cilium\n# LAST DEPLOYED: Sat May 25 00:02:42 2024\n# NAMESPACE: kube-system\n# STATUS: deployed\n# REVISION: 1\n# TEST SUITE: None\n# NOTES:\n# You have successfully installed Cilium with Hubble.\n \n# Your release version is 1.15.5.\n \n# For any further help, visit docs.cilium.io/en/v1.15/gettinghelp\n安裝完 CNI 後，叢集的狀態就會變成 Ready 了\n$ kubectl get nodes\n# NAME          STATUS   ROLES           AGE   VERSION\n# control-e01   Ready    control-plane   12m   v1.30.1\n# control-e02   Ready    control-plane   12m   v1.30.1\n# control-e03   Ready    control-plane   12m   v1.30.1\n# worker-e01    Ready    &lt;none&gt;          12m   v1.30.1\n# worker-e02    Ready    &lt;none&gt;          12m   v1.30.1\n\n大功告成，叢集已經啟用完成，可以開始部署應用程式了"},"Operations/Container/talos/talos-setup":{"title":"Talos 安裝紀錄 - 目錄","links":["Operations/Container/talos/talos-setup-01","Operations/Container/talos/talos-setup-02","Operations/Container/talos/talos-setup-03"],"tags":["talos","container/kubernetes","homelab"],"content":"目錄\n\nTalos 安裝紀錄 (1) - 簡介與環境\nTalos 安裝紀錄 (2) - 撰寫設定檔\nTalos 安裝紀錄 (3) - 安裝系統\n"},"Operations/Container/talos/talos-upgrade":{"title":"Talos 升級","links":[],"tags":["talos"],"content":"\n節錄：Upgrading Talos Linux | Talos Linux\n"},"Operations/Container/talos/talos-vip":{"title":"Talos - Virtual (shared) IP","links":[],"tags":["talos"],"content":"\n節錄：Virtual (shared) IP | Talos Linux\n\n簡介\n在多台 control plane 做高可用性 (high-availability; HA) 的環境下，能使用 VIP 使 client 可以透過這個 IP 連線到任何一台 control plane。\n運作方式\ncontrol plane 機器們會經由 etcd 投票，選出「一台」持有該 IP。如果該持有者消失或沒有回應，就會選出另一台機器。\n注意事項\n雖然可以使用 VIP 位置存取 Talos API，但 VIP 是由 etcd 運作的，故當機器的 etcd 壞掉、下線、或發生問題時，就沒辦法透過此 IP 連線機器修復。"},"Operations/Linux/eBPF":{"title":"eBPF","links":[],"tags":[],"content":"參考資料\n\neBPF Documentary\nModernizing BPF for the next 10 years [LWN.net]\n绿色记忆:eBPF学习笔记\n什麼是 eBPF ? An Introduction and Deep Dive into the eBPF Technology\nLinux 核心設計: 透過 eBPF 觀察作業系統行為 - HackMD\n"},"Operations/Linux/index":{"title":"Linux","links":[],"tags":[],"content":""},"Operations/Linux/virsh-autostart-failed":{"title":"使用 virtsh autostart 但 VM 不會自動啟動","links":[],"tags":["troubleshooting"],"content":"問題描述\n已經設定 sudo virsh autostart &lt;vm_name&gt;，但 VM 在重啟後一樣不會自動啟動\n系統環境\n\nFedora 40 with GNOME\n已經忘記當初 virsh 是怎麼裝，不過應該是跟著 cockpit virtual machine 外掛元件的指示做\n\n除錯經過\n以下每做完一個步驟都重新開機過\n\nsudo virsh dominfo &lt;vm_name&gt; 確定 autostart 是 enabled\nsudo systemctl disable libvirtd 後重新 enable\nsudo journalctl -xeu libvirtd.service 顯示 -- No entries --\n\n解決辦法\n依照 Not start after boot - Fedora Discussion 給的靈感，先執行\n$ systemctl list-unit-files &#039;*virt*&#039;\n# UNIT FILE                   STATE    PRESET  \n# libvirt-dbus.service        disabled disabled\n# libvirt-guests.service      disabled disabled\n# libvirtd.service            enabled  disabled\n# virtinterfaced.service      disabled disabled\n# virtlockd.service           disabled disabled\n# virtlogd.service            disabled disabled\n# virtlxcd.service            enabled  enabled \n# virtnetworkd.service        disabled  disabled\n# ......(略)\n挑出幾個感覺會用到的 enable\n# 看起來像是是虛擬網卡\n$ sudo systemctl enable virtnetworkd.service\n# 因為 VM 有用到硬體直通，所以也啟用它 \n$ sudo systemctl enable virtnodedevd.service\n# 看起來像是儲存空間相關的\n$ sudo systemctl enable virtstoraged.service\ntada 🎉  ，問題解決\n後記\nsudo systemctl status libvirtd 還是顯示 inactive (dead)，有點不解…"},"Operations/Network/VLAN":{"title":"VLAN","links":[],"tags":[],"content":"VLAN\n\n設定 VLAN 不代表對 Ports 的實體隔離\n不支援 Tagged VLAN 的交換機，這類封包會因為看不懂而自動丟棄\n\nTagged / Untagged Port\n封包的進出的視角為「交換機」：\n收到封包 → 入口規則 → 轉發 → 出口規則 → 發送封包\nTag\n\n封包帶 Tag，也就是帶 VLAN ID\n\nUntag\n\n封包沒有帶 Tag，也就是不帶 VLAN ID\n\nTagged Port\n\n封包離開 tagged port 時，會持續帶著標籤進入終端（出口規則）\n若該封包沒有帶 tag，進交換機時會被加上\n\nUntagged Port\n\n封包離開 untagged port 時，交換機會移除標籤後進入終端（出口規則）\n每個 port 只能有一個 untagged = PVID\n\nPVID (Port VLAN ID)\n\n封包出終端進入交換機時，如果有 tag，交換機會原封不動拿著 tag，若沒有，會貼上 PVID 的標籤（入口規則）\n封包出交換機的行為，看 tagged / untagged port（出口規則）\n\n參考資料\n\nSwitch基本觀念- PVID、VID、Tag\\Untag – weihanit-MIS記錄\nSwitch VLAN 設定教學 (V4.80) — Zyxel Community\n"},"Operations/Network/index":{"title":"Networking","links":[],"tags":[],"content":""},"Operations/Network/ipsec":{"title":"IPsec","links":[],"tags":["learn_with_gpt"],"content":"簡介\nIPsec 是一組通訊協定，一起用於在 OSI 模型的第 3 層（網路層）裝置之間設定安全連線\n通訊流程\nInternet Protocol Security (IPsec) 的通訊流程主要分為兩個階段：IKE（Internet Key Exchange）階段和數據傳輸階段\nIKE 階段\nInternet Key Exchange Protocol (IKE) 階段分為兩個子階段：IKE Phase 1 和 IKE Phase 2\nIKE Phase 1\n\n目標：建立雙方之間安全的通道並驗證彼此的身份，通常稱為 IKE SA（Security Association），用來保護後續的密鑰交換。這個階段有兩種模式：主模式（Main Mode）和快速模式（Aggressive Mode）\n流程\n\n協商加密和驗證參數 (IKE)：雙方協商使用的加密（如 AES、3DES）、Hash（如 SHA-256）、DH（Diffie-Hellman）組、身份驗證方法（如 PSK、證書）\nDH 密鑰交換：通過 DH 算法交換密鑰材料，生成共享秘密\n身份驗證：雙方根據約定的方法進行身份驗證\n\n\n主要參數\n\n加密演算法（Encryption Algorithm）： 用於加密 IKE 通訊的算法，例如 AES、3DES。\n雜湊（Hash Algorithm）： 用於確保數據完整性的算法，例如 SHA-1、SHA-256\nDH 群組（Diffie-Hellman Group）： 用於密鑰交換的 DH 群組，例如 Group 2、Group 14\n身份驗證方法（Authentication Method）： 用於驗證對方身份的方法，例如預共享密鑰（Pre-shared Key）、數位證書（Digital Certificates）\n\n\n\nIKE Phase 2\n\n目標：在 IKE Phase 1 建立的安全通道上協商用於保護實際數據流的參數，建立一個或多個 IPsec SA，這些 SA 將用於保護實際的數據傳輸。這個階段也稱為快速模式（Quick Mode）\n流程\n\n協商 IPsec SA（Security Association）：雙方協商用於保護數據流的加密和驗證參數\n建立 IPsec SA：雙方確認協商的參數並建立 IPsec SA\n\n\n主要參數\n\n加密演算法（Encryption Algorithm）： 與 IKE Phase 1 相同\n雜湊（Hash Algorithm）： 與 IKE Phase 1 相同\nPFS（Perfect Forward Secrecy）： 確保即使長期密鑰被破解，之前的通訊也不會被解密。PFS 需要再次執行 DH 密鑰交換\n\n\n\n數據傳輸階段\n在數據傳輸階段，IPsec 使用在 IKE Phase 2 中協商的 SA 來保護實際的數據傳輸。IPsec 有兩種工作模式：傳輸模式（Transport Mode）和隧道模式（Tunnel Mode）\n傳輸模式（Transport Mode）\n\n只對 IP 負載進行保護（即 IP 包的有效載荷）\n原始 IP 頭部保持不變，只是加密了數據部分\n主要用於主機間的通訊\n實際案例\n\n端點到端點的通訊\n\n描述： 兩個終端設備（例如兩台伺服器或兩個工作站）之間直接進行加密通訊\n應用情境： 企業內部的伺服器之間需要加密通訊，確保數據在傳輸過程中不被竄改或竊取。\n\n\n遠端桌面連接\n\n描述： 遠端桌面協定（RDP）使用 IPsec 傳輸模式來加密從用戶電腦到遠端伺服器的通訊。\n應用情境： IT 管理員通過遠端桌面連接到企業內部的伺服器進行維護和管理，需要保護傳輸過程中的敏感數據。\n\n\n\n\n\n\n隧道模式（Tunnel Mode）\n\n對整個 IP 封包進行保護，將其作為新 IP 封包的負載\n在原始 IP 封包外包裹一個新的 IP Header\n主要用於網關之間的通訊\n實際案例\n\n站點到站點 VPN（Site-to-Site VPN）\n\n描述： 兩個不同地理位置的網路（例如兩個分公司網路）之間通過 VPN 閘道器進行加密通訊\n應用情境： 公司總部與遠端分公司之間需要建立安全的通訊通道，以保護內部資料傳輸和應用程序存取\n\n\n遠端存取 VPN（Remote Access VPN）\n\n描述： 遠端工作人員通過 VPN 客戶端連接到企業網路，VPN 客戶端和 VPN 伺服器之間的通訊進行加密\n應用情境： 員工在外出差或在家工作時，需要安全地存取公司內部的應用和資料\n\n\n雲服務和數據中心之間的通訊\n\n描述： 企業內部數據中心和雲服務提供商之間使用 IPsec 隧道模式來保護數據傳輸\n應用情境： 企業將部分應用程序遷移到雲端，需要確保本地數據中心和雲端之間的通訊安全\n\n\n\n\n\n\n主要參數\n\nESP（Encapsulating Security Payload）： 提供數據加密和驗證。ESP 提供數據加密、來源驗證和數據完整性檢查。ESP 通過在數據包內部添加一個 ESP 標頭和尾部來保護數據。\nAH（Authentication Header）： 提供數據的完整性和身份驗證，但不加密數據。AH 會在數據包 header 添加 AH，用於承載身份驗證數據\nSPI（Security Parameter Index）： 用於標識不同的 IPsec SA\n密鑰材料（Keying Material）： 由 IKE 階段產生，用於加密和驗證數據\n\n其他補充\n\nSA（Security Association）：IPsec 中定義的雙方之間的安全參數集合，包括 SPI（Security Parameter Index）、加密演算法、hash 等\n隧道模式：指定 IPsec 工作於傳輸模式還是隧道模式\n生存時間（Lifetime）：SA 的有效期，在到期後需要重新協商\n\nPolicy-based vs. Route-based\nPolicy-based IPsec\n\n特點\n\n配置方式： 基於安全策略（Security Policies）來決定哪些流量需要經過 IPSec 隧道\n工作原理： 通過配置安全策略，定義特定的流量（例如，特定的源 IP、目的 IP、協定、端口等）需要被加密和保護。當匹配到這些策略的流量時，流量會被引導進入 IPSec 隧道\n使用場景： 通常用於需要精細控制和高安全性的環境，例如企業內部的敏感數據傳輸\n\n\n優點\n\n精細控制：可以對特定流量進行精確控制和保護\n安全性高：能夠為特定的應用和數據流量提供定制的安全策略\n\n\n缺點\n\n複雜性：配置和管理較為複雜，需要精細設置安全策略\n擴展性：隨著網路規模的擴大，管理和維護策略的工作量也會增加\n\n\n\nRoute-based IPsec\n\n特點\n\n配置方式： 基於路由來決定哪些流量需要經過 IPsec 隧道。\n工作原理： 配置虛擬的網路介面（通常稱為 VTI，Virtual Tunnel Interface），然後通過靜態路由或動態路由協議（例如 BGP、OSPF）將流量路由到這些介面。所有經過這些介面的流量都會自動被加密和保護\n使用場景： 通常用於需要簡化配置和擴展性強的環境，例如大型企業的分支機構互聯\n\n\n優點\n\n簡化配置：通過路由表來控制流量，配置和管理相對簡單\n擴展性好：適合於大型網路，隨著網路規模的擴大，配置和管理相對容易\n\n\n缺點\n\n控制精度較低：不能像 Policy-based IPsec 一樣對特定流量進行精細控制\n路由依賴：需要依賴路由協議和路由表來控制流量\n\n\n\n總結\n\nPolicy-based IPsec： 使用安全策略來決定哪些流量需要經過 IPsec 隧道，適用於需要精細控制和高安全性的環境\nRoute-based IPsec： 使用路由來決定哪些流量需要經過 IPsec 隧道，適用於需要簡化配置和高擴展性的環境\n\n這兩種配置方法各有優缺點，具體選擇取決於網路的需求和規模。通常來說，Policy-based IPsec 提供更高的控制精度，而 Route-based IPsec 提供更好的擴展性和管理便利性。\n參考資料\n\nChatGPT\nManual:IP/IPsec - MikroTik Wiki\nUniFi Gateway - Site-to-Site IPsec VPN with Third-Party Gateways (Advanced) – Ubiquiti Help Center\n"},"Operations/Network/mikrotik/routeros-fasttrack":{"title":"RouterOS Fasttrack","links":["Operations/Network/mikrotik/routeros-firewall"],"tags":["mikrotik","firewall"],"content":"簡介\nFasttrack 是用來節約 CPU 的使用，因為不是所有封包都要一直進出防火牆，要不是被轉發、放行、就是被丟棄\n當一個封包進入 RouterOS 時， 會先進入防火牆 ，一直到他碰到 fasttrack 規則\n如果符合 fasttrack 規則該封包就會被標記，之後這個封包就會被快速放行，不會再經過其他防火牆規則、Queue 等等\n參考資料\n\nRouterOS上的QoS 1:服务质量队列原理简介 - YouTube\n\n28:00 有提到 fasttrack 的觀念\n\n\nQueue and Fasttrack - MikroTik\nMangle and Fasttrack [SOLVED] - MikroTik\n"},"Operations/Network/mikrotik/routeros-firewall":{"title":"RouterOS 防火牆 (Firewall)","links":[],"tags":[],"content":"每一個 chain 都會進入一次防火牆，chian 有好幾種如: input, forward, output, prerouting, postrouting。"},"Operations/Network/mikrotik/routeros-qos":{"title":"RouterOS QoS 與 Queue","links":["Operations/Network/mikrotik/routeros-traffic-shaping","Operations/Network/mikrotik/routeros-fasttrack"],"tags":["mikrotik","qos"],"content":"QoS\nQuality of Service 服務品質，藉由犧牲一部分流量品質，保障重要流量的服務品質，像是遊樂園的快速通關券、頭等艙和經濟艙的排隊\n常見的方式有：\n\n頻寬保留\n頻寬限制\n流量優先度\n流量塑型 (Traffic Shaping)\n\nQueue (佇列)的參數\n\nqueue (佇列) 長度 (bucket)\nqueue (佇列) 數量\n排隊方式\n放行方式\n\n優先放行\n循環放行\n隨機放行\n\n\n\nRouterOS 中的 Queue\n\nSimple Queue\n\n搭配 mangle 標記，也可以只搭配 IP/Port\n雙向流量限制、時間限制\n優先級設定\n\n\nQueue Tree\n\n搭配 mangle 標記，設定放行方式\n細分\n\n\nInterface Queue\nQueue Type（自定義）\n有硬體 queue，就不需要丟給處理器處理了\n\nQueue 的類型\n\nFIFO (First In First Out) 先進先出\n\nBFIFO：根據 Byte\nPFIFO：根據 Packet\nMQ PFIFO：Multi Queue\nPCQ：Per Connection Queue\n\n\n\n設定 QoS 的前置工作\n\n關閉相對應的 Fasttrack 規則\n知道上下載頻寬的上限\n\n有可能頻寬不穩定，那可能要少設一點，不然 QoS 會變得沒有意義，他會不知道自己的 Queue 到底會不會被消化完\n設定太少可能永遠用不完頻寬\n\n\n了解需求的流量類型\n\n遊戲？影片？網頁？下載？VoIP？\n不能有漏網之魚\n\n\n\n參考資料\n\nMikroTik RouterOS QoS (Queue/HTB/PCQ) - Mobile01\nRouterOS QoS. Queue / Hierarchical Token Bucket / Per… | by Mason Lyu | MischievousBOSS | Medium\nRouterOS上的QoS 1:服务质量队列原理简介 - YouTube\nMikroTik@台灣 | 新手提問 | Facebook\n"},"Operations/Network/mikrotik/routeros-traffic-shaping":{"title":"Mikrotik RouterOS 流量塑型 (Traffic Shaping)","links":[],"tags":["mikrotik","qos"],"content":"情境\n假使有一個突發流量超出了頻寬，在沒有 QoS 的時候，超出的部分會被丟棄。\n若加上了 QoS，超出的部分就會被存到佇列中，等到頻寬足夠後發送。\n但如此以來會造成高延遲，不適用於 UDP 傳輸類的資料，如語音通話、遊戲等。\n簡介\nTraffic Shaping 是一種流量管理技術，用於控制進出網路的資料傳輸速度，主要目的是平滑流量峰值，避免瞬間的大量流量導致網路擁塞。Traffic Shaping 的實現方式通常包括：\n\n頻寬限制：通過設定最大速率來限制某些類型的流量\n突發允許：允許短時間內超過設定的頻寬限制，以處理突發的流量需求\n封包排隊：將資料流排隊處理，根據設定的速率發送\n\n排隊太長封包就要等很久，對延遲敏感的應用就會受到影響\n\n\n\n與 QoS 的關係\nTraffic Shaping 是實現 QoS 的一種技術手段之一。通過控制流量速率，可以確保高優先級流量在網路擁塞時仍能得到保證。\nTraffic Shaping 負責控制總體流量速率，QoS 則負責優先級和資源分配。兩者協同工作，確保網路流量得到合理的控制和優先處理。"},"Operations/Proxmox/index":{"title":"Proxmox","links":[],"tags":[],"content":"一些有用的 Proxmox 網站們\n\nProxmox VE Helper-Scripts\n"},"Operations/Proxmox/proxmox-nut":{"title":"Proxmox 使用 NUT 自動關機","links":["Utilities/ups"],"tags":["ups","nut","proxmox","homelab"],"content":"依照設定 UPS Client完成後，Proxmox 就會讓 VM Guest 和 Host 優雅關機 1\nFootnotes\n\n\nProxomox - NUT Client.md · GitHub ↩\n\n\n"},"Operations/Utilities/index":{"title":"Utilities","links":[],"tags":[],"content":""},"Operations/Utilities/soap-age-encryption":{"title":"使用 SOPS 和 AGE 加密基礎設施設定檔","links":[],"tags":["security","devops/devsecops","iac","homelab","opensource"],"content":"簡介\n在 IaC (Infrastructure as Code) 盛行的時代，任何基礎設施都可以寫成檔案，方便上版控管理每一次更動。檔案中不免會有機敏資訊，此時可以將資訊抽出寫到 .env，再將其丟入 .gitignore 排除。\nAGE\nAGE 是一個簡單的加密工具，不需要任何設定直接產生一組加解密金鑰，並且可以將金鑰分享給其他人。AGE 也可以將加密過的檔案傳送給其他人，只要對方有金鑰就可以解密。\n\nage is a simple, modern and secure file encryption tool, format, and Go library.\nIt features small explicit keys, no config options, and UNIX-style composability.\n\nSOPS\nSOPS 全名為 Secrets OPerationS，他提供了另一個方式讓我們儲存基礎設施設定檔。根據官方敘述：\n\nSOPS is an editor of encrypted files that supports YAML, JSON, ENV, INI and BINARY formats and encrypts with AWS KMS, GCP KMS, Azure Key Vault, age, and PGP.\n\n簡單來說，SOPS 可以幫助我們加密 YAML、JSON 等等常見的資料格式，也可以加密 AWS、GCP 之類的雲端服務設定。不過有趣的是 SOPS 加密不是對整個檔案，key-value 中的 key 會保留，value 才會被處理。\n輕鬆上手\nmacOS 可以使用 homebrew 安裝兩者套件\n$ brew install sops age\nAGE\n產生公私鑰\n$ age-keygen -o age.key\n# Public key: age1d3k30aqxamz66323d464es42caw5jf32mnew5pdktnv4rxv6npws3sxfc8\n我們可以查看 age 產生的金鑰內容\n$ cat age.key\n# # created: 2024-05-21T14:34:59+08:00\n# # public key: age1d3k30aqxamz66323d464es42caw5jf32mnew5pdktnv4rxv6npws3sxfc8\n# AGE-SECRET-KEY-1PM05035R5PKU7YXVYMCPXX9D3CASVM5AG3Z0PHVRY8WZYQNG0GSQ36QHQ5\n這樣我們就可以使用這把金鑰，搭配 SOPS 加密檔案了\nSOPS\n先設置環境變數\n$ export SOPS_AGE_KEY_FILE=&quot;${PWD}/age.key&quot;\n假設有一個檔案叫做 secrets.yaml\n$ cat secrets.yaml\n# cluster:\n#     id: buUIh....\n#     secret: khlN....\n# secrets:\n#     bootstraptoken: qwt....\n#     secretboxencryptionsecret: 3kGCa....\n# trustdinfo:\n#     token: 5kyafg.ce2cod17...\n# certs:\n# ...\n加密檔案中每一個值，並覆蓋原始內容\n$ sops --encrypt -i --age $(cat age.key | sed -n &#039;s/^# public key: \\(.*\\)/\\1/p&#039;) secrets.yaml\n此時檔案會變成這樣\nsecrets.yamlcluster:\n    id: ENC[AES256_GCM,data:9b2...=,tag:L...==,type:str]\n    secret: ENC[AES256_GCM,data:KoA...4=,tag:S4D...==,type:str]\nsecrets:\n    bootstraptoken: ENC[AES256_GCM,data:EKm...=,tag:gh...==,type:str]\n    secretboxencryptionsecret: ENC[AES256_GCM,data:zbrq...=,tag:iOb...==,type:str]\ntrustdinfo:\n    token: ENC[AES256_GCM,data:x2b...=,tag:adV...==,type:str]\ncerts:\n...\nsops:\n    kms: []\n    gcp_kms: []\n    azure_kv: []\n    hc_vault: []\n    age:\n        - recipient: age12c...\n          enc: |\n            -----BEGIN AGE ENCRYPTED FILE-----\n            YWdlLW...==\n            -----END AGE ENCRYPTED FILE-----\n    lastmodified: &quot;2024-05-21T06:52:59Z&quot;\n    mac: ENC[AES256_GCM,data:x5Xts6...=,tag:7wt...==,type:str]\n    pgp: []\n    unencrypted_suffix: _unencrypted\n    version: 3.8.1\n在加密的狀態下查看檔案\n$ sops talsecret.sops.yaml\n解密檔案，並覆蓋原始內容\n$ sops --decrypt -i talsecret.sops.yaml\n其他參考資料\n\nUsing SOPS with Age and Git like a Pro\n"},"Operations/Utilities/ups":{"title":"使用 NUT 控管 UPS 設備","links":[],"tags":["ups","nut","opensource","homelab","raspberry-pi"],"content":"\n內文大多來自 使用樹莓派監測控管UPS設備／Network UPS Tools／Linux [2023年最新] - Kyle’s Blog (KodeLab)，該文章寫的非常好，請大家至原文閱讀，以下為整理的筆記\n\nNUT 以及他的好朋友們\nNUT 元件\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n元件說明nut-driver與 UPS 溝通的驅動程式nut-server能透過 nut-driver 獲得資訊後傳遞給 clientnut-client (nut-monitor)能連上 nut-server 獲取 UPS 資訊，然後控制本機關機等操作nut-cgi簡易的網頁介面，顯示 UPS 狀態\nNUT 指令\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n指令說明upsdNUT Server 的 deamonupsdrvctlNUT Server 上的 UPS 驅動程式upsc簡易版的 UPS Client，可以快速連上本地或網路上的 NUT Server 查看 UPS 狀態upsmon監看 UPS 與關機控制的程式upscmd對 UPS 下達指令，例如開始電池測試、關閉蜂鳴器…等upslog查看 UPS 狀態紀錄\nNUT 設定檔\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n設定檔說明nut.confNUT 運作模式等基本設定ups.confNUT 的 UPS Driver 要連線的 UPS 的設定upsd.confNUT 的 UPS Daemon 的設定upsd.usersnut-server 使用者的帳密upsmon.confnut-monitor 的設定upssched.confNUT 排程hosts.confNUT CGI Server 要監控的端點\n設定 NUT Server\n\n筆者安裝的環境是 Raspberry Pi 3B+，UPS 型號是 CP1500PFCLCDa TW，搭配串接另一台 Synology NAS UPS Server\n\n安裝 NUT UPS Server\n# 安裝 nut 後會一併安裝 nut-server 跟 nut-client 等程式。\n$ sudo apt install -y nut\n# 或是 Fedora 系列的\n$ sudo dnf install -y nut\n抓取連線的 UPS\n將 UPS 的 USB 連接線插入電腦中\n$ lsusb\n# Bus 001 Device 005: ID 0764:0601 Cyber Power System, Inc. PR1500LCDRT2U UPS\n# Bus 001 Device 004: ID 0424:7800 Microchip Technology, Inc. (formerly SMSC)\n# Bus 001 Device 003: ID 0424:2514 Microchip Technology, Inc. (formerly SMSC) USB 2.0 Hub\n# Bus 001 Device 002: ID 0424:2514 Microchip Technology, Inc. (formerly SMSC) USB 2.0 Hub\n# Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n$ sudo nut-scanner -U\n# Scanning USB bus.\n# [nutdev1]\n# \tdriver = &quot;usbhid-ups&quot;\n#\tport = &quot;auto&quot;\n#\tvendorid = &quot;0764&quot;\n#\tproductid = &quot;0601&quot;\n#\tproduct = &quot;CP1500PFCLCDa TW&quot;\n#\tserial = &quot;CXXXX2000111&quot;\n#\tvendor = &quot;CPS&quot;\n#\tbus = &quot;001&quot;\n設定 UPS 連線\nsudo nano /etc/nut/ups.conf，將設定加入到檔案末\n/etc/nut/ups.conf[server_ups]\n  driver = usbhid-ups\n  port = auto\n  vendorid = 0764\n  productid = 0601\n  pollinterval = 5\n  desc = &quot;Server Cluster UPS&quot;\n  serial = CXXXX2000111\n\n行 1：UPS 的名字，可以自己取\n行 2-5、8：依照前步驟 sudo nut-scanner -U 顯示的內容填入\n行 6：可不填入，範例的意思是每五秒拉一次訊息\n行 7：UPS 的描述，內容可自行填入\n\n完成後，執行 sudo upsdrvctl restart 重啟驅動\n設定 NUT 模式\nsudo nano /etc/nut/nut.conf，將 none 改為 netserver\n/etc/nut/nut.confMODE=netserver\nMODE 還有：單機模式 standalone、僅連線 ups server 的 netclient\n完成後執行 sudo systemctl restart nut-server 重啟服務\n設定 NUT 網路\nsudo nano /etc/nut/upsd.conf，將設定加入到檔案末\n/etc/nut/upsd.confLISTEN 0.0.0.0 3493\n3493 是預設 port，0.0.0.0 代表允許任何 IP 連入，若要增加安全性可以稍作修改\n設定 NUT 帳號\nsudo nano /etc/nut/upsd.users，將設定加入到檔案末\n/etc/nut/upsd.users[admin]\n\tpassword = your_admin_password\n\tactions = SET\n\tinstcmds = ALL\n \n[upsmon]\n\tpassword = your_password\n\tupsmon primary\n\n行 1：admin 可以改成自己喜歡的帳號名，這個是使用 upscmd 控制 UPS 的時候會使用到的帳號\n行 6：upsmon 是給 nut-monitor 使用的帳號\n行 8：電源不足時，secondary 會先關機，primary 後作動，因為他是最主要的系統。舊版本可能稱為 master、slave\n\n完成後執行 sudo systemctl restart nut-server 重啟服務\n設定 NUT CGI Server\n因為筆者使用 Raspberry Pi 跑 NUT，不想要浪費寶貴的計算資源，決定使用 nut-cgi 和 cgi 用的網頁伺服器 lighttpd\n$ sudo apt install nut-cgi lighttpd\nsudo nano /etc/nut/hosts.conf，將設定加入到檔案末\n/etc/nut/hosts.confMONITOR server_ups@localhost &quot;Server Cluster UPS&quot;\nMONITOR ups@192.168.1.1 &quot;Network Device UPS&quot;\n\n行 2：筆者有另一台 UPS 接在 Synology NAS 上，故填入兩行\n\n設定完成後可以讓 cgi server 跑起來\n$ sudo lighttpd-enable-mod cgi\n$ sudo service lighttpd force-reload\n這時候進入 http://your_ip/cgi-bin/nut/upsstats.cgi 就能看到簡潔有利的畫面了\n\n\n不喜歡那麼簡陋畫面的話，還有 Brandawg93/PeaNUT、rshipp/webNUT、BeardedTek-com/nutui 可以選擇\n使用 NUT\n查看 UPS 狀態\n使用 upsc 即可查看狀態，upsc ups_name@host。Ex: upsc server_ups@localhost\nUPS 資訊\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n名稱說明數值範例battery.charge目前電池電量（%）100battery.charge.warning電池低電量警告閾值（%）20battery.charge.low電池低電量保護閾值（%）10battery.runtime預估電池剩餘供電時間（秒）3525battery.type電池類型PbAcidbattery.voltage電池電壓27.6battery.voltage.nominal標準電壓值24device.type設備類型（NUT 可以檢測 UPS 以外的一些電源設備）upsdriver.name驅動名稱usbhid-upsdriver.parameter.port與 UPS 的通訊埠autodriver.version驅動版本2.7.4input.frequency輸入的交流電頻率60input.voltage輸入到 UPS 的電壓（V）115.0input.frequency輸入到 UPS 的交流電頻率input.transfer.high電壓上限，超過時會啟動穩壓保護136input.transfer.low電壓下限，低於時會啟動穩壓保護88output.voltage目前輸出給用電設備的電壓115.0ups.mfrUPS 製造商CPSups.modelUPS 型號CP1500PFCLCDups.vendoridUPS 製造商 ID0764ups.productidUPS 型號 ID0501ups.loadUPS 負載 (%)10ups.temperatureUPS 溫度ups.statusUPS 狀態，常見有 OL（正常）、OB（使用電池）、OB（低電量）、RB（電池測試）、CHRG（電池充電）OLups.beeper.statusUPS 蜂鳴器是否啟動enabledups.test.resultUPS 自我測試結果：No test initiated、In progress、Aborted、Done and passed…等Done and passed\n控制 UPS\n使用 upscmd 即可控制 UPS，下 -l 可以看自己能對 UPS 做什麼事，ex: upscmd -u admin -p password -l ups@localhost\n帳號是稍早寫在 /etc/nut/upsd.users 中的， 若是連線到 Synology NAS 的 UPS，帳號密碼分別是 monuser 和 secret\n關閉 UPS 蜂鳴器的範例：upscmd -u monuser -p secret ups@nas.ip beeper.disable\nUPS 日誌\nupslog 可以用來查看 ups 日誌\n$ sudo upslog -s server_ups -i 5 -l -\n# Network UPS Tools upslog 2.8.0\n# logging status of server_ups to - (5s intervals)\n# Init SSL without certificate database\n# 20240617 215506 100 117.0 13 [OL] NA NA\n# 20240617 215511 100 117.0 13 [OL] NA NA\n# 20240617 215513 100 117.0 13 [OL] NA NA\n\n-i 5：每五秒一次\n-l -：輸出到 STDOUT\n輸出格式：yyyyMMdd HHmmss 電池電量 輸入電壓 負載(%) [供電狀態] UPS溫度 輸入頻率\n\n設定 NUT Client\n\n這裡 client 指的是遠端機器，跟上方 NUT Server 不同台\n\n安裝 NUT Client\n$ sudo apt install -y nut-client\n設定 NUT 模式\nsudo nano /etc/nut/nut.conf，將 none 改為 netclient\n/etc/nut/nut.confMODE=netclient\n確認連線正常\n先使用 upsc 連線看看，確認中間通訊沒有被防火牆擋住\n$ upsc server_ups@192.168.1.2\n# Init SSL without certificate database\n# battery.charge: 100\n# battery.charge.low: 10\n# battery.charge.warning: 20\n# battery.mfr.date: CPS\n# battery.runtime: 3525\n# .......(略)\n設定 NUT 監控的機器\nsudo nano /etc/nut/upsmon.conf，將設定加入到檔案末\n/etc/nut/upsmon.confMONITOR server_ups@192.168.1.2 1 upsmon your_password secondary\n\n將 upsmon 以及 your_password 改成您的帳號及密碼，該值是在 upsd.users 中設定的\n語法解釋：MONITOR &lt;system&gt; &lt;powervalue&gt; &lt;username&gt; &lt;password&gt; (&quot;primary&quot;|&quot;secondary&quot;)\n\n&lt;system&gt;：語法是 &lt;upsname&gt;@&lt;hostname&gt;[:&lt;port&gt;]\n&lt;powervalue&gt;：通常為 1\nprimary 代表會最後關機，因為他是最主要的系統，會先讓 secondary 先關機\n\n\n還有很多設定，upsmon.conf 中的註解寫的非常清楚\n\nSHUTDOWNCMD：關機要執行的指令\nNOTIFYCMD：發生事件要執行的指令\nNOTIFYFLAG：發生什麼事件要做什麼事情，語法：NOTIFYFLAG &lt;notify type&gt; &lt;flag&gt;[+&lt;flag&gt;][+&lt;flag&gt;] ...\n\n&lt;notify type&gt;\n\nONLINE：UPS 在線上，即市電恢復時會觸發\nONBATT：UPS 使用電池供電，即市電中斷時會觸發\nLOWBATT：UPS 低電量時會觸發\nFSD：UPS 正在被關閉 (Forced Shutdown)\nCOMMOK：與 nut-server 成功建立連接時觸發\nCOMMBAD：與 nut-server 建立連接失敗（或斷線）時觸發\nSHUTDOWN：UPS 發出關機指令觸發\nREPLBATT：UPS 需要更換電池時觸發\nNOCOMM：無法與 UPS 建立連接 (UPS未就緒) 時觸發\n\n\n&lt;flag&gt;\n\nSYSLOG：只在 syslog 中紀錄\nWALL：在終端上提示 (/bin/wall)\nEXEC：執行 NOTIFYCMD 指定的命令，並傳遞相關事件\nIGNORE：忽略該事件\n\n\n\n\n\n\n\n設定完成後重啟服務 upsmon -c reload，並且可以用 ps 查看服務是否正常執行\n$ upsmon -c reload\n$ ps -ef | grep upsmon\n# root     1143160       1  0 11:22 ?        00:00:00 /lib/nut/upsmon start\n# nut      1143161 1143160  0 11:22 ?        00:00:02 /lib/nut/upsmon start\n# root     1250473 1242800  0 22:00 pts/0    00:00:00 grep upsmon"},"Operations/index":{"title":"🛠️ 維運 (Operations)","links":[],"tags":[],"content":""},"Programming/index":{"title":"👨‍💻 開發 (Programming)","links":[],"tags":[],"content":""},"index":{"title":"歡迎來到 KaiYi 的筆記本","links":[],"tags":[],"content":"你好你好 👋 \nKaiYi 目前是一位台科大資訊管理碩士班學生，擅長網頁與雲原生應用開發、DevOps、以及網路安全。\n這裡是他的個人筆記本，主要用來記錄學習中未經整理的筆記，可能有點雜亂、細節沒有寫清楚，有任何問題歡迎互相交流。他的 email 是：me@kaiyeee.tw，或是想要 casual 一點的話，可以聯絡他的 Telegram (@kaiyeee)。\n如果文章中有任何錯誤，請不要吝嗇直接聯絡他！\n希望你會喜歡這裡 🥰 "},"tags/learn_with_gpt":{"title":"🔖 Learn with GPT","links":[],"tags":[],"content":""}}